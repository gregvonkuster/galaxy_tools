

<?xml version="1.0"?>
<job_conf>
    <plugins workers="8">
        <!-- "workers" is the number of threads for the runner's work queue.
                                                    The default from <plugins> is used if not defined for a <plugin>.
          -->
        <plugin id="cli" type="runner" load="galaxy.jobs.runners.cli:ShellJobRunner" />
        <plugin id="local" type="runner" load="galaxy.jobs.runners.local:LocalJobRunner"/>
        <plugin id="slurm" type="runner" load="galaxy.jobs.runners.slurm:SlurmJobRunner">
		<param id="drmaa_library_path">/usr/lib/slurm-drmaa/lib/libdrmaa.so.1</param>
        </plugin>
        <plugin id="dynamic" type="runner">
            <!-- The dynamic runner is not a real job running plugin and is
                                                                    always loaded, so it does not need to be explicitly stated in
                 <plugins>. However, if you wish to change the base module
                 containing your dynamic rules, you can do so.

                 The `load` attribute is not required (and ignored if
                 included).
            -->
            <param id="rules_module">galaxy.jobs.rules</param>
        </plugin>
    </plugins>
    <handlers default="handlers">
        <!-- Additional job handlers - the id should match the name of a
                                                    [server:<id>] in galaxy.ini.
         -->
        <handler id="handler0" tags="handlers"/>
        <handler id="handler1" tags="handlers"/>
        <handler id="handler2" tags="handlers"/>
        <handler id="handler3" tags="handlers"/>
        <handler id="handler4" tags="handlers"/>
        <handler id="handler5" tags="handlers"/>
        <handler id="handler6" tags="handlers"/>
        <handler id="handler7" tags="handlers"/>
        <!-- Handlers will load all plugins defined in the <plugins> collection
                                                    above by default, but can be limited to a subset using <plugin>
             tags. This is useful for heterogenous environments where the DRMAA
             plugin would need to be loaded more than once with different
             configs.
         -->
        <!--
                                               <handler id="special_handler0" tags="special_handlers"/>
        <handler id="special_handler1" tags="special_handlers"/>
        <handler id="trackster_handler"/>
        -->
    </handlers>
    <destinations default="local">
        <!-- Destinations define details about remote resources and how jobs
                                                    should be executed on those remote resources.
         -->
        <destination id="local" runner="slurm">
            <env id="_JAVA_OPTIONS">-Djava.io.tmpdir=/home/galaxy/galaxy/database/files/tmp -Xmx8192m -Xms256m</env>
            <param id="nativeSpecification">--nodes=1 --ntasks=1</param>
        </destination>

        <destination id="local_queue_wf" runner="slurm">
            <env id="_JAVA_OPTIONS">-Djava.io.tmpdir=/home/galaxy/galaxy/database/files/tmp -Xmx8192m -Xms256m</env>
            <param id="nativeSpecification">--nodes=1 --ntasks=1</param>
        </destination>

        <destination id="local_multicore" runner="slurm">
            <env id="_JAVA_OPTIONS">-Djava.io.tmpdir=/home/galaxy/galaxy/database/files/tmp -Xmx65536m -Xms256m</env>
            <param id="nativeSpecification">--nodes=1 --ntasks=8</param>
        </destination>

        <destination id="dynamic_processors_select" runner="dynamic">
            <param id="type">python</param>
            <param id="function">dynamic_processors_select</param>
        </destination>

    </destinations>
    <resources default="local">
      <!-- Group different parameters defined in job_resource_params_conf.xml
                                            together and assign these groups ids. Tool section below can map
           tools to different groups. This is experimental functionality!
      -->
      <group id="default"></group>
      <group id="memoryonly">memory</group>
      <!-- <group id="all">processors,memory,time,project</group> -->
      <group id="all">processors,memory,time</group>
      <group id="processors">processors</group>
    </resources>
    <tools>
        <!-- Tools can be configured to use specific destinations or handlers,
                                                    identified by either the "id" or "tags" attribute.  If assigned to
             a tag, a handler or destination that matches that tag will be
             chosen at random.
         -->
        <!-- <tool id="upload1" handler="handler0" destination="local" /> -->
        <!-- <tool id="bar" destination="dynamic"/> -->
        <!-- Next example defines resource group to insert into tool interface
                                                    and pass to dynamic destination (as resource_params argument). -->
        <!-- <tool id="longbar" destination="dynamic" resources="all" /> -->
        <!-- <tool id="baz" handler="special_handlers" destination="bigmem"/> -->
        <!-- <tool id="plant_tribes_assembly_post_processor" destination="dynamic_processors_select" resources="processors" /> -->
	<!--
	<tool id="vsnp_add_zero_coverage" destination="dynamic_processors_select" resources="processors" />
	<tool id="vsnp_build_tables" destination="dynamic_processors_select" resources="processors" />
	<tool id="vsnp_determine_ref_from_data" destination="dynamic_processors_select" resources="processors" />
	<tool id="vsnp_get_snps" destination="dynamic_processors_select" resources="processors" />
        -->
	<tool id="vsnp_add_zero_coverage" destination="local_multicore" />
	<tool id="vsnp_build_tables" destination="local_multicore" />
	<tool id="vsnp_determine_ref_from_data" destination="local_multicore" />
	<tool id="vsnp_get_snps" destination="local_multicore" />
    </tools>
    <limits>
        <!-- Certain limits can be defined. The 'concurrent_jobs' limits all
                                                    control the number of jobs that can be "active" at a time, that
             is, dispatched to a runner and in the 'queued' or 'running'
             states.

             A race condition exists that will allow destination_* concurrency
             limits to be surpassed when multiple handlers are allowed to
             handle jobs for the same destination. To prevent this, assign all
             jobs for a specific destination to a single handler.
        -->
        <!-- registered_user_concurrent_jobs:
                                                       Limit on the number of jobs a user with a registered Galaxy
                account can have active across all destinations.
        -->
        <limit type="registered_user_concurrent_jobs">50</limit>
        <!-- anonymous_user_concurrent_jobs:
                                                       Likewise, but for unregistered/anonymous users.
        -->
        <limit type="anonymous_user_concurrent_jobs">1</limit>
        <!-- destination_user_concurrent_jobs:
                                                       The number of jobs a user can have active in the specified
                destination, or across all destinations identified by the
                specified tag. (formerly: concurrent_jobs)
        -->
        <limit type="destination_user_concurrent_jobs" id="local">20</limit>
        <limit type="destination_user_concurrent_jobs" tag="mycluster">50</limit>
        <limit type="destination_user_concurrent_jobs" tag="longjobs">20</limit>
        <!-- destination_total_concurrent_jobs:
                                                       The number of jobs that can be active in the specified
                destination (or across all destinations identified by the
                specified tag) by any/all users.
        -->
        <limit type="destination_total_concurrent_jobs" id="local">100</limit>
        <limit type="destination_total_concurrent_jobs" tag="longjobs">20</limit>
        <limit type="destination_total_concurrent_jobs" id="local_queue_wf">1</limit>
        <!-- walltime:
                                                       Amount of time a job can run (in any destination) before it
                will be terminated by Galaxy.
         -->
        <limit type="walltime">192:00:00</limit>
        <!-- output_size:
                                                       Size that any defined tool output can grow to before the job
                will be terminated. This does not include temporary files
                created by the job. Format is flexible, e.g.:
                '10GB' = '10g' = '10240 Mb' = '10737418240'
        -->
        <limit type="output_size">200GB</limit>
    </limits>
    <macros>
        <xml name="foohost_destination" tokens="id,walltime,ncpus">
            <destination id="@ID@" runner="cli">
                <param id="shell_plugin">SecureShell</param>
                <param id="job_plugin">Torque</param>
                <param id="shell_username">galaxy</param>
                <param id="shell_hostname">foohost_destination.example.org</param>
                <param id="job_Resource_List">walltime=@WALLTIME@,ncpus=@NCPUS@</param>
            </destination>
        </xml>
        <xml name="multi_tool" tokens="id">
            <tool id="@ID@" destination="local_multicore" />
        </xml>
    </macros>
</job_conf>

