<?xml version="1.0"?>
<!-- A sample job config that describes all available options -->
<job_conf>
    <plugins workers="4">
        <!-- "workers" is the number of threads for the runner's work queue.
             The default from <plugins> is used if not defined for a <plugin>.
             For all asynchronous runners (i.e. everything other than
             LocalJobRunner), this is the number of threads available for
             starting and finishing jobs. For the LocalJobRunner, this is the
             number of concurrent jobs that Galaxy will run.
          -->
        <plugin id="local" type="runner" load="galaxy.jobs.runners.local:LocalJobRunner"/>
        <plugin id="dynamic" type="runner">
            <!-- The dynamic runner is not a real job running plugin and is
                 always loaded, so it does not need to be explicitly stated in
                 <plugins>. However, if you wish to change the base module
                 containing your dynamic rules, you can do so.

                 The `load` attribute is not required (and ignored if
                 included).
            -->
            <param id="rules_module">galaxy.jobs.rules</param>
        </plugin>
        <!-- Pulsar runners (see more at https://pulsar.readthedocs.io/) -->
        <plugin id="pulsar_runner" type="runner" load="galaxy.jobs.runners.pulsar:PulsarMQJobRunner">
          <!-- AMQP URL to connect to. -->
          <param id="amqp_url">pyamqp://galaxy:sCoYMPG2nbZSTT7UqkyqtAHv8@galaxy.cryoem.psu.edu:5672/galaxy</param>
          <!-- URL remote Pulsar apps should transfer files to this Galaxy
               instance to/from. This can be unspecified/empty if
               galaxy_infrastructure_url is set in galaxy.yml.
          -->
          <param id="galaxy_url">https://galaxy.cryoem.psu.edu</param>
          <!-- AMQP does not guarantee that a published message is received by
               the AMQP server, so Galaxy/Pulsar can request that the consumer
               acknowledge messages and will resend them if acknowledgement is
               not received after a configurable timeout.  -->
          <param id="amqp_acknowledge">False</param>
          <!-- Galaxy reuses Pulsar's persistence_directory parameter (via the
               Pulsar client lib) to store a record of received
               acknowledgements, and to keep track of messages which have not
               been acknowledged. --> 
          <param id="persistence_directory">/usr/local/galaxy/galaxy/database/files/persisted_data</param> 
          <!-- Number of seconds to wait for an acknowledgement before
               republishing a message. -->
          <param id="amqp_ack_republish_time">1200</param>
          <!-- Pulsar job manager to communicate with (see Pulsar
               docs for information on job managers). -->
          <param id="manager">_default_</param>
          <!-- The AMQP client can provide an SSL client certificate (e.g. for
               validation), the following options configure that certificate
               (see for reference:
                 https://kombu.readthedocs.io/en/latest/reference/kombu.connection.html
               ). If you simply want to use SSL but not use/validate a client
               cert, just use the ?ssl=1 query on the amqp URL instead. 
	  <param id="amqp_connect_ssl_ca_certs">/etc/ssl/certs/incommon-ca.crt</param> 
	  <param id="amqp_connect_ssl_keyfile">/etc/ssl/private/galaxy.key</param> 
	  <param id="amqp_connect_ssl_certfile">/etc/ssl/certs/galaxy.crt</param>
           <param id="amqp_connect_ssl_cert_reqs">cert_required</param> -->
          <!-- By default, the AMQP consumer uses a nonblocking connection with
               a 0.2 second timeout. In testing, this works fine for
               unencrypted AMQP connections, but with SSL it will cause the
               client to reconnect to the server after each timeout. Set to a
               higher value (in seconds) (or `None` to use blocking connections). -->
          <param id="amqp_consumer_timeout">2.0</param>
        </plugin>
    </plugins>
    <handlers>
        <!-- Job handler processes - for a full discussion of job handlers, see the documentation at:
             https://docs.galaxyproject.org/en/latest/admin/scaling.html

             For documentation on handler assignment methods, see the documentation under:
             https://docs.galaxyproject.org/en/latest/admin/scaling.html#job-handler-assignment-methods

             The <handlers> container tag takes three optional attributes:

               <handlers assign_with="method" max_grab="count" default="id_or_tag"/>

               - `assign_with` - How jobs should be assigned to handlers. The value can be a single method or a
                 comma-separated list that will be tried in order. The default depends on whether any handlers and a job
                 handler "pool" (such as uWSGI mules in a `job-handlers` farm) are configured. Valid methods are
                 explained in detail in the documentation referenced above and have the values:

                   - `mem-self` - In-memory Self Assignment
                   - `db-self`- Database Self Assignment
                   - `db-preassign` - Database Preassignment
                   - `uwsgi-mule-message` - uWSGI Mule Messaging
                   - `db-transaction-isolation` - Database Transaction Isolation
                   - `db-skip-locked` - Database SKIP LOCKED

               - `max_grab` - Limit the number of jobs that a handler will self-assign per jobs-ready-to-run check loop
                 iteration. This only applies for methods that cause handlers to self-assign multiple jobs at once
                 (db-skip-locked, db-transaction-isolation) and the value is an integer > 0. Default is to grab as many
                 jobs ready to run as possible.

               - `default` - An ID or tag of the handler(s) that should handle any jobs not assigned to a specific
                 handler (which is probably most of them). If unset, the default is any untagged handlers plus any
                 handlers in the `job-handlers` (no tag) pool.
        -->
        <!-- Explicitly defined job handlers - the id should match the handler process's `server_name`. For webless
             handlers, this is the value of the `server-name` argument to `galaxy-main`.
         -->
        <handler id="handler0"/>
        <!-- <handler id="handler1"/> -->
        <!-- Handlers will load all plugins defined in the <plugins> collection
             above by default, but can be limited to a subset using <plugin>
             tags. This is useful for heterogenous environments where the DRMAA
             plugin would need to be loaded more than once with different
             configs.
        -->
        <!-- Handlers are grouped by defining (comma-separated) tags -->
        <handler id="special_handler0" tags="special_handlers"/>
        <handler id="special_handler1" tags="special_handlers"/>
    </handlers>
    <destinations default="pulsar">
        <!-- Destinations define details about remote resources and how jobs
             should be executed on those remote resources.
         -->
        <destination id="local" runner="local"/>
        <destination id="dynamic" runner="dynamic">
            <!-- A destination that represents a method in the dynamic runner.

                 foo should be a Python function defined in any file in
                 lib/galaxy/jobs/rules.
            -->
            <param id="function">foo</param>
        </destination>
        <destination id="pulsar" runner="pulsar_runner" >
            <!-- The RESTful Pulsar client sends a request to Pulsar
                 to populate various system properties. This
                 extra step can be disabled and these calculated here
                 on client by uncommenting jobs_directory and
                 specifying any additional remote_property_ of
                 interest, this is not optional when using message
                 queues.
            -->
            <param id="default_file_action">remote_transfer</param>
            <param id="dependency_resolution">remote</param>
            <param id="jobs_directory">/storage/galaxy/jobs/staging</param>
            <param id="persistence_directory">/storage/galaxy/jobs/persisted_data</param>
            <param id="remote_metadata">False</param>
            <param id="rewrite_parameters">True</param>
            <param id="transport">curl</param>
            <!-- Otherwise MQ and Legacy pulsar destinations can be supplied
                 all the same destination parameters as the RESTful client documented
                 above (though url and private_token are ignored when using a MQ).
            -->
        </destination>
        <!-- Jobs can be re-submitted for various reasons (to the same destination or others,
             with or without a short delay). For instance, jobs that hit the walltime on one
             destination can be automatically resubmitted to another destination. Re-submission
             is defined on a per-destination basis using ``resubmit`` tags.

             Multiple `resubmit` tags can be defined, the first resubmit condition that is true
             (i.e. evaluates to a Python truthy value) will be used for a particular job failure.

             The ``condition`` attribute is optional, if not present, the
             resubmit destination will be used for all relevant failure types.
             Conditions are expressed as Python-like expressions (a fairly safe subset of Python
             is available). These expressions include math and logical operators, numbers,
             strings, etc.... The following variables are available in these expressions:

               - "walltime_reached" (True if and only if the job runner indicates a walltime maximum was reached)
               - "memory_limit_reached" (True if and only if the job runner indicates a memory limit was hit)
               - "unknown_error" (True for job or job runner problems that aren't otherwise classified)
               - "attempt" (the re-submission attempt number this is)
               - "seconds_since_queued" (the number of seconds since the last time the job was in a queued state within Galaxy)
               - "seconds_running" (the number of seconds the job was in a running state within Galaxy)

             The ``handler`` attribute is optional, if not present, the job's original
             handler will be reused for the resubmitted job. The ``destination`` attriubte
             is optional, if not present the job's original destination will be reused for the
             re-submission. The ``delay`` attribute is optional, if present it will cause the job to
             delay for that number of seconds before being re-submitted. 
         -->
    </destinations>
    <resources default="default">
      <!-- Group different parameters defined in job_resource_params_conf.xml
           together and assign these groups ids. Tool section below can map
           tools to different groups. This is experimental functionality!
      -->
      <group id="default"></group>
      <group id="memoryonly">memory</group>
      <group id="all">processors,memory,time,project</group>
    </resources>
    <tools>
        <!-- Tools can be configured to use specific destinations or handlers,
             identified by either the "id" or "tags" attribute.  If assigned to
             a tag, a handler or destination that matches that tag will be
             chosen at random.
         -->
        <tool id="Show tail1" destination="pulsar"/>
        <tool id="Count1" destination="pulsar"/>
        <tool id="upload1" destination="local"/>
        <tool id="bwa" handler="handler0">
        </tool>
        <tool id="bowtie" handler="handler1">
        </tool>
        <tool id="bar" destination="dynamic"/>
        <!-- Next example defines resource group to insert into tool interface
             and pass to dynamic destination (as resource_params argument). -->
        <tool id="longbar" destination="dynamic" resources="all" />
        <!-- Pick a handler randomly from those declaring this tag. -->
        <tool id="baz" handler="special_handlers" destination="bigmem"/>
        <tool id="foo" handler="handler0">
            <param id="source">trackster</param>
        </tool>
    </tools>
    <limits>
        <!-- Certain limits can be defined. The 'concurrent_jobs' limits all
             control the number of jobs that can be "active" at a time, that
             is, dispatched to a runner and in the 'queued' or 'running'
             states.

             A race condition exists that will allow destination_* concurrency
             limits to be surpassed when multiple handlers are allowed to
             handle jobs for the same destination. To prevent this, assign all
             jobs for a specific destination to a single handler.
        -->
        <!-- registered_user_concurrent_jobs:
                Limit on the number of jobs a user with a registered Galaxy
                account can have active across all destinations.
        -->
        <limit type="registered_user_concurrent_jobs">2</limit>
        <!-- anonymous_user_concurrent_jobs:
                Likewise, but for unregistered/anonymous users.
        -->
        <limit type="anonymous_user_concurrent_jobs">1</limit>
        <!-- destination_user_concurrent_jobs:
                The number of jobs a user can have active in the specified
                destination, or across all destinations identified by the
                specified tag. (formerly: concurrent_jobs)
        -->
        <limit type="destination_user_concurrent_jobs" id="local">1</limit>
        <limit type="destination_user_concurrent_jobs" tag="mycluster">2</limit>
        <limit type="destination_user_concurrent_jobs" tag="longjobs">1</limit>
        <!-- destination_total_concurrent_jobs:
                The number of jobs that can be active in the specified
                destination (or across all destinations identified by the
                specified tag) by any/all users.
        -->
        <limit type="destination_total_concurrent_jobs" id="local">16</limit>
        <limit type="destination_total_concurrent_jobs" tag="longjobs">100</limit>
        <!-- walltime:
                Amount of time a job can run (in any destination) before it
                will be terminated by Galaxy.
         -->
        <limit type="walltime">24:00:00</limit>
        <!-- total_walltime:
                Total walltime that jobs may not exceed during a set period.
                If total walltime of finished jobs exceeds this value, any
                new jobs are paused.  `window` is a number in days,
                representing the period.
         -->
        <limit type="total_walltime" window="30">24:00:00</limit>
        <!-- output_size:
                Size that any defined tool output can grow to before the job
                will be terminated. This does not include temporary files
                created by the job. Format is flexible, e.g.:
                '10GB' = '10g' = '10240 Mb' = '10737418240'
        -->
        <limit type="output_size">10GB</limit>
    </limits>
    <macros>
        <xml name="foohost_destination" tokens="id,walltime,ncpus">
            <destination id="@ID@" runner="cli">
                <param id="shell_plugin">SecureShell</param>
                <param id="job_plugin">Torque</param>
                <param id="shell_username">galaxy</param>
                <param id="shell_hostname">foohost_destination.example.org</param>
                <param id="job_Resource_List">walltime=@WALLTIME@,ncpus=@NCPUS@</param>
            </destination>
        </xml>
    </macros>
</job_conf>

